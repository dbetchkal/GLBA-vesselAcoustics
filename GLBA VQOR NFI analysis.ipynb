{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3f07dd",
   "metadata": {},
   "source": [
    "### NFI Cumulative Distribution Function plots and statistics for the Glacier Bay VQOR\n",
    "These for GLBARENDU2011, GLBAMCLEOD2011 (re-analyzed from original [Lynch E. 2012. *Glacier Bay National Park and Preserve: Acoustical Monitoring 2011*](https://irma.nps.gov/DataStore/Reference/Profile/2268564)), and subsequent monitoring GLBARENDU2021, GLBAMCLEOD2021. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# general NSNSD acoustical tools\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\DBetchkal\\PythonScripts\\3_GITHUB_REPOSITORIES\\iyore\")\n",
    "sys.path.append(r\"C:\\Users\\DBetchkal\\PythonScripts\\3_GITHUB_REPOSITORIES\\soundDB\")\n",
    "sys.path.append(r\"C:\\Users\\DBetchkal\\PythonScripts\\3_GITHUB_REPOSITORIES\\derivedDataFunctions\")\n",
    "sys.path.append(r\"C:\\Users\\DBetchkal\\PythonScripts\\3_GITHUB_REPOSITORIES\\GLBA-vesselAcoustics\")\n",
    "import iyore\n",
    "from soundDB import *\n",
    "from derivedDataFunctions import *\n",
    "from merge_SRCID import *\n",
    "from AIS_utilities import contiguous_regions\n",
    "\n",
    "\n",
    "def VOQR_cumulative_histogram_nfi(ds, unit, site, year):\n",
    "    \n",
    "    '''\n",
    "    Analyze spectrogram annotations for a site to make a CDF plot.\n",
    "    Work under conditions of the GLBA Vessel Operating and Quota Requirements plan.\n",
    "    '''\n",
    "\n",
    "    src = srcid(ds, unit=unit, site=site, year=year).combine()\n",
    "    src_m = merge_SRCID(src) # merging is important because vessels often span multiple hours\n",
    "\n",
    "    # extract only vessel events; a condition of the VQoR\n",
    "    # in this analysis: \n",
    "    #     AIS-derived vessels use a 4-character srcID mapped to MMSI (e.g., '0.702')\n",
    "    #     other vessels have the conventional srcID (i.e., '3.0')\n",
    "    vessel_events = src_m.loc[(src_m.srcID == 3)|(src_m.srcID.astype('str').str.len() == 5), :].copy()\n",
    "\n",
    "    # they must be sorted\n",
    "    vessel_events.sort_index(inplace=True)\n",
    "    \n",
    "\n",
    "    # this is by far more straightforward \n",
    "    NFIlst = vessel_events.index.to_series().diff()\n",
    "\n",
    "    # we only want non-negative intervals\n",
    "    out = pd.Series(np.array([m for m in NFIlst[NFIlst > \"00:00:00\"]]))\n",
    "\n",
    "    \n",
    "    # compute noise-free intervals between all events\n",
    "    nfis = NFI_list(vessel_events, unit=\"hours\")\n",
    "\n",
    "    # for comparing sites a constant number of bins is required; let's use 100\n",
    "    val, base = np.histogram(nfis, bins=np.logspace(-2,2, 200))\n",
    "\n",
    "    #evaluate the cumulative\n",
    "    cumu = np.cumsum(val)/max(np.cumsum(val))\n",
    "    \n",
    "    # compute time audible as a percentage of the overall record\n",
    "    # note: this computation assumes partial days have been separated at the archive level\n",
    "    TA_p = 100*src_m.loc[:, \"len\"].sum().total_seconds()/(3600*len([e for e in ds.nvspl(unit=u, site=s, year=y)]))\n",
    "    \n",
    "    print(\"80th = {0:.1f} hours\".format(np.percentile(nfis, 80)),\n",
    "          \"70th = {0:.1f} hours\".format(np.percentile(nfis, 70)),\n",
    "          \"mean = {0:.1f} hours\".format(np.mean(nfis)),\n",
    "          \"50th = {0:.1f} hours\".format(np.percentile(nfis, 50)),\n",
    "          \"\\n\\n\")\n",
    "    \n",
    "    return base, cumu, TA_p, metrics\n",
    "\n",
    "def VOQR_cumulative_histogram_sel(ds, unit, site, year):\n",
    "    \n",
    "    '''\n",
    "    Analyze spectrogram annotations for a site to make a CDF plot.\n",
    "    Work under conditions of the GLBA Vessel Operating and Quota Requirements plan.\n",
    "    '''\n",
    "\n",
    "    src = srcid(ds, unit=unit, site=site, year=year).combine()\n",
    "    src = src[~src.index.duplicated(keep='first')]\n",
    "    src_m = merge_SRCID(src) # merging is important because vessels often span multiple hours\n",
    "\n",
    "    # extract only vessel events; a condition of the VQoR\n",
    "    # in this analysis: \n",
    "    #     AIS-derived vessels use a 4-character srcID mapped to MMSI (e.g., '0.702')\n",
    "    #     other vessels have the conventional srcID (i.e., '3.0')\n",
    "    vessel_events = src_m.loc[(src_m.srcID == 3)|(src_m.srcID.astype('str').str.len() == 5), :].copy()\n",
    "\n",
    "    # they must be sorted\n",
    "    vessel_events.sort_index(inplace=True)\n",
    "    \n",
    "    sels = [float(val) for val in vessel_events['SELt'].values]\n",
    "    \n",
    "    # for comparing sites a constant number of bins is required; let's use 100\n",
    "    val, base = np.histogram(sels, bins=np.linspace(20, 130, 200))\n",
    "\n",
    "    #evaluate the cumulative\n",
    "    cumu = np.cumsum(val)/max(np.cumsum(val))\n",
    "    \n",
    "    return base, cumu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c70a0d",
   "metadata": {},
   "source": [
    "### Step One: plot overall vessel NFIs, SELs as Cumulative Distribution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef6a77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = iyore.Dataset(r\"E:\")\n",
    "\n",
    "wdir = r\"T:\\ResMgmt\\WAGS\\Sound\\Experiments & Data\\2022 - 2021 GLBA VQOR Vessel Quotas and Operating Requirements plan\"\n",
    "\n",
    "# =========== plot ===================\n",
    "\n",
    "nfi_refs = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100] # in hours\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3.5))\n",
    "\n",
    "#                   ('GLBA', 'BARTC', 2020, 'mediumblue'), # where are these?\n",
    "#                   ('GLBA', 'HUTCH', 2020, 'pink'),\n",
    "# for u, s, y, c in [('GLBA', 'BARTC', 2021, 'blue'),\n",
    "#                    ('GLBA', 'MCLEOD', 2011, 'mediumvioletred'),\n",
    "#                    ('GLBA', 'MCLEOD', 2021, 'violet'),\n",
    "#                    ('GLBA', 'RENDU', 2011, 'mediumspringgreen'),\n",
    "#                    ('GLBA', 'RENDU', 2020, 'orange'),\n",
    "#                    ('GLBA', 'RENDU', 2021, 'green'),\n",
    "#                    ('GLBA', 'REID', 2020, 'indigo'),\n",
    "#                    ('GLBA', 'BEARD', 2021, 'saddlebrown'),\n",
    "#                    ('GLBA', 'HUTCH', 2011, 'orangered'),\n",
    "#                    ('GLBA', 'MOUSE', 2011, 'dodgerblue')]:\n",
    "    \n",
    "for u, s, y, c in [('GLBA', 'RENDU', 2011, 'mediumspringgreen'),\n",
    "                   ('GLBA', 'RENDU', 2021, 'green'),\n",
    "                   ('GLBA', 'MCLEOD', 2011, 'mediumvioletred'),\n",
    "                   ('GLBA', 'MCLEOD', 2021, 'violet')]:\n",
    "    \n",
    "    print(\"now processing\", u+s+str(y))\n",
    "    \n",
    "    # to visualize the annotations we: parse + filter + mine + represent...\n",
    "    base, cumu, TA_p, metrics = VOQR_cumulative_histogram_nfi(ds, u, s, y)\n",
    "\n",
    "    # plot the results\n",
    "    ax.step(base[:-1], cumu, label=u+s+str(y), \n",
    "            lw=0.75, color=c)\n",
    "    \n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(\"Noise Free Interval length, hours \\n[logarithmic bins]\", \n",
    "              labelpad=25, fontsize=11)\n",
    "ax.set_xticks(ticks=nfi_refs)\n",
    "ax.set_xticklabels(labels=nfi_refs)\n",
    "ax.set_xlim([0.1, 100])\n",
    "\n",
    "# add visual cues for familiar lengths of time\n",
    "# (this helps interpret the logarithmic bins)\n",
    "for t in nfi_refs: \n",
    "    ax.axvline(t, lw=0.5, alpha=0.1, color=\"k\", zorder=-2)\n",
    "\n",
    "ax.set_ylabel(\"Percentile\", labelpad=25, fontsize=11)\n",
    "ax.set_ylim([0, 1.01])\n",
    "\n",
    "ax.axhline(1, lw=0.5, ls=\"dotted\", alpha=0.1, color=\"k\", zorder=-2)\n",
    "\n",
    "ax.axhline(0.8, lw=0.5, ls=\"dotted\", alpha=0.6, color=\"k\", zorder=-2)\n",
    "ax.axhline(0.7, lw=0.5, ls=\"dotted\", alpha=0.6, color=\"k\", zorder=-2)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.1, 0.9818),\n",
    "          frameon=False)\n",
    "\n",
    "ax.set_title(\"Noise Free Interval Between Vessel Events\",\n",
    "             loc=\"center\", y=1.12, fontsize=13)\n",
    "\n",
    "plt.savefig(wdir+os.sep+\"GLBA_VOQR_VesselNFIs_MCLEOD_20221104.png\", dpi=200,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff799902",
   "metadata": {},
   "source": [
    "#### Same graphic as above, but for Sound Exposure Level (SEL)\n",
    "The analysis is identical, but visualizing the results requires different contextualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = iyore.Dataset(r\"E:\")\n",
    "\n",
    "wdir = r\"T:\\ResMgmt\\WAGS\\Sound\\Experiments & Data\\2022 - 2021 GLBA VQOR Vessel Quotas and Operating Requirements plan\"\n",
    "\n",
    "# =========== plot ===================\n",
    "\n",
    "nfi_refs = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100] # in hours\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3.5))\n",
    "\n",
    "for u, s, y, c in [(\"GLBA\", \"RENDU\", 2011, 'mediumspringgreen'),\n",
    "                   (\"GLBA\", \"RENDU\", 2021, 'green'),\n",
    "                   (\"GLBA\", \"MCLEOD\", 2011, 'mediumvioletred'),\n",
    "                   (\"GLBA\", \"MCLEOD\", 2021, 'violet'),\n",
    "                   (\"GLBA\", \"BARTC\", 2021, 'dodgerblue'),\n",
    "                   (\"GLBA\", \"BEARD\", 2021, 'chocolate')]:\n",
    "    \n",
    "    print(\"now processing\", u+s+str(y))\n",
    "    \n",
    "    # to visualize the annotations we: parse + filter + mine + represent...\n",
    "    base, cumu,  = VOQR_cumulative_histogram_sel(ds, u, s, y)\n",
    "    \n",
    "    # plot the results\n",
    "    ax.step(base[:-1], cumu, label=u+s+str(y), \n",
    "            lw=0.75, color=c)\n",
    "\n",
    "# ax.set_xscale('log')\n",
    "ax.set_xlabel(\"Sound Exposure Level [12.5 - 1250 Hz] (dB)\", \n",
    "              labelpad=25, fontsize=11)\n",
    "# ax.set_xticks(ticks=nfi_refs)\n",
    "# ax.set_xticklabels(labels=nfi_refs)\n",
    "ax.set_xlim([30, 100])\n",
    "\n",
    "# # add visual cues for familiar lengths of time\n",
    "# # (this helps interpret the logarithmic bins)\n",
    "# for t in nfi_refs: \n",
    "#     ax.axvline(t, lw=0.5, alpha=0.1, color=\"k\", zorder=-2)\n",
    "\n",
    "ax.set_ylabel(\"Percentile\", labelpad=25, fontsize=11)\n",
    "ax.set_ylim([0, 1.01])\n",
    "\n",
    "# ax.axhline(1, lw=0.5, ls=\"dotted\", alpha=0.1, color=\"k\", zorder=-2)\n",
    "# ax.axhline(0.8, lw=0.5, ls=\"dotted\", alpha=0.6, color=\"k\", zorder=-2)\n",
    "# ax.axhline(0.7, lw=0.5, ls=\"dotted\", alpha=0.6, color=\"k\", zorder=-2)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.1, 0.9818),\n",
    "          frameon=False)\n",
    "\n",
    "ax.set_title(\"Sound Exposure Level (=Noise Dose) of Vessel Events\",\n",
    "             loc=\"center\", y=1.12, fontsize=13)\n",
    "\n",
    "plt.savefig(wdir+os.sep+\"GLBA_VOQR_VesselSELsfirstDraft.png\", dpi=200,\n",
    "            bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a800584",
   "metadata": {},
   "source": [
    "### Step Two: Load and filter the annotation record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = iyore.Dataset(r\"E:\")\n",
    "u, s, y = \"GLBA\", \"RENDU\", 2021\n",
    "\n",
    "src = srcid(ds, unit=u, site=s, year=y).combine()\n",
    "src_m = merge_SRCID(src) # merging is important because vessels often span multiple hours\n",
    "\n",
    "# extract only vessel events; a condition of the VQoR\n",
    "# in this analysis: \n",
    "#     AIS-derived vessels use a 4-character srcID mapped to MMSI (e.g., '0.702')\n",
    "#     other vessels have the conventional srcID (i.e., '3.0')\n",
    "vessel_events = src_m.loc[(src_m.srcID == 3)|(src_m.srcID.astype('str').str.len() == 5), :].copy()\n",
    "\n",
    "# they must be sorted\n",
    "vessel_events.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e38cc0",
   "metadata": {},
   "source": [
    "### Step Three: Load NVSPL record, add a binary series for vessel noise presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (part of) the acoustic record using `soundDB`\n",
    "nv = nvspl(ds, unit=u, site=s, year=y).combine()\n",
    "nv.index = nv.index.droplevel()\n",
    "\n",
    "# select only the salient data we'll be working with\n",
    "nv_select = nv.loc[:, \"12.5\":\"dbA\"].merge(nv[\"WindSpeed\"], how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "# we want to expand all the events into a complete list of their inclusive datetimes\n",
    "noisy_times = np.array([])\n",
    "for start, row in vessel_events.iterrows():\n",
    "    \n",
    "    event_times = [start + dt.timedelta(seconds=i) for i in np.arange(0, row[\"len\"].total_seconds())]\n",
    "    noisy_times = np.append(noisy_times, event_times)\n",
    "\n",
    "\n",
    "# flatten and take only the unique values, as this is a composite effort\n",
    "noise_series_raw = noisy_times.flatten()\n",
    "noise_series = np.unique(noise_series_raw)\n",
    "\n",
    "# compile into a `pandas.Series` object for merging with NVSPL\n",
    "noise_presence = pd.Series(np.ones(noise_series.shape), index=noise_series, name=\"presence\")\n",
    "\n",
    "# merge the indicator series with the acoustic data, \n",
    "# keeping all rows from the latter and assigning np.nan for all times without noise\n",
    "indicator_nv = nv_select.merge(noise_presence, how=\"left\", left_index=True, right_index=True)\n",
    "indicator_nv = indicator_nv.fillna(0) # then convert np.nan -> 0\n",
    "\n",
    "indicator_nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfis_select=contiguous_regions(indicator_nv['presence'] == 0)\n",
    "I_max = (nfis_select.T[1] - nfis_select.T[0]).max()/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43931f",
   "metadata": {},
   "source": [
    "### Step Four: Compute longest diurnal NFI\n",
    "- group NVSPL record by date, then <br>\n",
    "- subselect by desired diural period, then <br>\n",
    "- group `indicator_nv['presence']` by contiguous regions with value `== 0`, that is, *noise-free seconds*\n",
    "- thus: *length of group* $\\equiv$ *length of noise-free interval*\n",
    "- glean `max(len(group))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_bound = 5 # \"05:00\"\n",
    "end_bound = 21 # \"21:00\" # must be in the same day!\n",
    "\n",
    "I_maxs = []\n",
    "nvspl_dates = np.unique(indicator_nv.index.date)\n",
    "for date in nvspl_dates:\n",
    "    \n",
    "    s_time = dt.time(hour=start_bound)\n",
    "    s_bound = dt.datetime.combine(date, s_time)\n",
    "    e_time = dt.time(hour=end_bound)\n",
    "    e_bound = dt.datetime.combine(date, e_time)\n",
    "    \n",
    "    print(\"on\", date, \"between\", s_time, \"and\", e_time, \":\\n\")\n",
    "    print(\"\\tthe longest noise-free interval\")\n",
    "    \n",
    "    nv_select = indicator_nv.loc[s_bound:e_bound, :]\n",
    "\n",
    "    try:\n",
    "        nfis_select=contiguous_regions(nv_select['presence'] == 0)\n",
    "        I_max = (nfis_select.T[1] - nfis_select.T[0]).max()/3600\n",
    "        print(\"\\tI_max = {0:.01f} hours\".format(I_max))\n",
    "        print(\"\\n\\n\")\n",
    "        I_maxs.append(I_max)\n",
    "        \n",
    "    except IndexError:\n",
    "        print(\"no noise-free periods during the timeframe indicated! \\n\\n\")\n",
    "        I_maxs.append(np.nan)\n",
    "\n",
    "longest = pd.DataFrame(np.array([I_maxs]).T, \n",
    "                       index=nvspl_dates, columns=[\"I_max(hrs)\"])\n",
    "longest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60cb77e",
   "metadata": {},
   "source": [
    "### Step Five: plot longest-NFI results for a deployment\n",
    "Scale to length of diurnal period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_len = end_bound-start_bound\n",
    "\n",
    "plt.figure(figsize=(12, 2.5))\n",
    "plt.plot(longest.index, longest[\"I_max(hrs)\"], marker=\"o\", ms=3, color=\"k\", ls=\"\")\n",
    "plt.axhline(period_len, ls=\"--\", color=\"gray\", alpha=0.2, zorder=-1)\n",
    "plt.text(longest.index.min(), period_len+0.8, \n",
    "         \"maximum achievable length = {0:.1f} hrs\".format(period_len), \n",
    "         color=\"gray\", alpha=0.4, zorder=-1)\n",
    "plt.ylim([0, period_len + 4])\n",
    "plt.ylabel(\"Maximum Noise-Free\\nInterval Observed (hrs)\", labelpad=15)\n",
    "plt.title(u+s+str(y)+\" maximum diurnal NFI\", loc=\"left\", y=1.04)\n",
    "plt.savefig(r\"C:\\Users\\DBetchkal\\Desktop\\MaximumDiurnalNFI_\"+u+s+str(y)+\".png\",\n",
    "            dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "# format tabular report\n",
    "print(\"n days observed: {0} days\".format(len(longest)), \"\\n\",\n",
    "      \"min observed: {0:.1f} hours\".format(longest[\"I_max(hrs)\"].min()), \"\\n\",\n",
    "      \"20th percentile: {0:.1f} hours\".format(longest[\"I_max(hrs)\"].quantile(0.2)), \"\\n\",\n",
    "      \"median: {0:.1f} hours\".format(longest[\"I_max(hrs)\"].quantile(0.5)), \"\\n\",\n",
    "      \"mean: {0:.1f} hours\".format(longest[\"I_max(hrs)\"].mean()), \"\\n\",\n",
    "      \"80th percentile: {0:.1f} hours\".format(longest[\"I_max(hrs)\"].quantile(0.8)), \"\\n\",\n",
    "      \"maximum achievable: {0:.1f} hours\".format(period_len), \"\\n\", \n",
    "      \"was achieved?\", (longest[\"I_max(hrs)\"] >= period_len).sum() > 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
